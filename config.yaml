#######################
# Python Environments #
#######################

viton_hd_python: ~/anaconda3/envs/VITON_HD/bin/python
cihp_python: ~/anaconda3/envs/CIHP_PGN/bin/python
background_python: ~/anaconda3/envs/adelai/bin/python

########
# Data #
########

data_dir: /data/VITON/datasets/VITON_AIHub/
label_dir: /data/VITON/datasets/VITON_AIHub/test/image-parse/
save_dir: ./results/

dataset_dir: ./inputs/
image_dir: ./inputs/image/
pose_dir: ./inputs/openpose-img/
pose_json_dir: ./inputs/openpose-json/
cloth_dir: ./inputs/cloth/
cloth_mask_dir: ./inputs/cloth_mask/
data_list: test_pairs.txt

############
# CondInst #
############

background_dir: ./background/

#############################################
# CIHP (Crowd Instance-Level Human Parsing) #
#############################################

cihp_pgn_dir: ./CIHP-PGN/
cihp_checkpoint: checkpoint/CIHP_pgn
cihp_input_dir: ./inputs
cihp_list_path: ./inputs/val.txt
cihp_data_id_list: ./inputs/val_id.txt
cihp_output_dir: ./output/cihp_parsing
cihp_ref_img: ./inputs/palette_ref.png

#############################
# VITON-HD (Virtual Try-On) #
#############################

viton_hd_dir: ./VITON-HD/
viton_hd_dataset_list: test_pairs.txt
viton_hd_output_dir: ./output/viton-hd
which_epoch: latest

batch_size: 8,
num_workers: 4,
lr: 0.0002,
beta1: 0.5, # momentum term of adam
load_height: 512,
load_width: 512,
shuffle: True,
save_mid_results: True,
semantic_nc: 13, # number of human-parsing map classes
pool_size: 0, # the size of image buffer that stores previously generated images

# SegGen
lambda_ce: 10,
seg_options: {
    beta1_G: 0.5,
    beta2_G: 0.5,
    beta1_D: 0.999,
    beta2_D: 0.999,
    gan_mode: ls # ls, original, w, hinge
},

# GMM
grid_size: 5,
lambda_const: 0.04,
gmm_options: {
    beta1_G: 0.5,
    beta2_G: 0.999
},

# ALIAS
norm_G: spectralaliasinstance,
ngf: 64, # number of generator filters in the first conv layer
num_upsampling_layers: most, # normal, more, most.
# If \'more\', add upsampling layer between the two middle resnet blocks.
# If \'most\', also add one more (upsampling + resnet) layer at the end of the generator.
lambda_feat: 10,
lambda_percept: 10,
norm_feat: 128,
alias_options: {
    beta1_G: 0,
    beta2_G: 0.9,
    beta1_D: 0,
    beta2_D: 0.9,
    n_conv: 8,
    gan_mode: hinge # ls, original, w, hinge
},

print_freq: 1000 # frequency of showing training results on console
display_freq: 10000 # frequency of showing results on console


######################
# Background Merging #
######################

background_info: ./inputs/backgrounds.txt
background_step1_output_dir: ./output/background_step1
background_step1_input_dir: ./inputs/org_img
background_final_output_dir: ./output/final

############
# OpenPose #
############

openpose_dir: ./openpose/

###############
# Checkpoints #
###############
checkpoint_dir: ./checkpoint/
seg_checkpoint: blackened_entire_image_seg_train/200_net_G.pth
gmm_checkpoint: blackened_entire_image_gmm_train/200_net_G.pth
alias_checkpoint: blackened_entire_image_alias_train/200_net_G.pth


